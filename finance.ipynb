{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yfinance langchain_pinecone openai python-dotenv langchain-community sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import json\n",
    "import yfinance as yf\n",
    "import concurrent.futures\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_info(symbol: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves and formats detailed information about a stock from Yahoo Finance.\n",
    "\n",
    "    Args:\n",
    "        symbol (str): The stock ticker symbol to look up.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing detailed stock information, including ticker, name,\n",
    "              business summary, city, state, country, industry, and sector.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {os.getenv(\"YAHOO_ACCESS_TOKEN\")}'\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    data = yf.Ticker(symbol, session=session)\n",
    "    # data = yf.Ticker(symbol)\n",
    "    stock_info = data.info\n",
    "\n",
    "    properties = {\n",
    "        \"Ticker\": stock_info.get('symbol', 'Information not available'),\n",
    "        'Name': stock_info.get('longName', 'Information not available'),\n",
    "        'Business Summary': stock_info.get('longBusinessSummary'),\n",
    "        'City': stock_info.get('city', 'Information not available'),\n",
    "        'State': stock_info.get('state', 'Information not available'),\n",
    "        'Country': stock_info.get('country', 'Information not available'),\n",
    "        'Industry': stock_info.get('industry', 'Information not available'),\n",
    "        'Sector': stock_info.get('sector', 'Information not available'),\n",
    "        'Market Cap': stock_info.get('marketCap', 'Information not available'),\n",
    "        'Volume': stock_info.get('volume', 'Information not available'),\n",
    "    }\n",
    "\n",
    "    return properties\n",
    "    # return stock_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the two sentences: 0.6133\n"
     ]
    }
   ],
   "source": [
    "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the given text using a specified Hugging Face model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to generate embeddings for.\n",
    "        model_name (str): The name of the Hugging Face model to use.\n",
    "                          Defaults to \"sentence-transformers/all-mpnet-base-v2\".\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(text)\n",
    "\n",
    "\n",
    "def cosine_similarity_between_sentences(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two sentences.\n",
    "\n",
    "    Args:\n",
    "        sentence1 (str): The first sentence for similarity comparison.\n",
    "        sentence2 (str): The second sentence for similarity comparison.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity score between the two sentences,\n",
    "               ranging from -1 (completely opposite) to 1 (identical).\n",
    "\n",
    "    Notes:\n",
    "        Prints the similarity score to the console in a formatted string.\n",
    "    \"\"\"\n",
    "    # Get embeddings for both sentences\n",
    "    embedding1 = np.array(get_huggingface_embeddings(sentence1))\n",
    "    embedding2 = np.array(get_huggingface_embeddings(sentence2))\n",
    "\n",
    "    # Reshape embeddings for cosine_similarity function\n",
    "    embedding1 = embedding1.reshape(1, -1)\n",
    "    embedding2 = embedding2.reshape(1, -1)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(embedding1, embedding2)\n",
    "    similarity_score = similarity[0][0]\n",
    "    print(f\"Cosine similarity between the two sentences: {similarity_score:.4f}\")\n",
    "    return similarity_score\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sentence1 = \"I like walking to the park\"\n",
    "sentence2 = \"I like running to the office\"\n",
    "\n",
    "similarity = cosine_similarity_between_sentences(sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_tickers():\n",
    "    \"\"\"\n",
    "    Downloads and parses the Stock ticker symbols from the GitHub-hosted SEC company tickers JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing company tickers and related information.\n",
    "\n",
    "    Notes:\n",
    "        The data is sourced from the official SEC website via a GitHub repository:\n",
    "        https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\n",
    "    \"\"\"\n",
    "    # URL to fetch the raw JSON file from GitHub\n",
    "    url = \"https://raw.githubusercontent.com/team-headstart/Financial-Analysis-and-Automation-with-LLMs/main/company_tickers.json\"\n",
    "\n",
    "    # Making a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content directly\n",
    "        company_tickers = json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "        # Optionally save the content to a local file for future use\n",
    "        with open(\"company_tickers.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(company_tickers, file, indent=4)\n",
    "\n",
    "        print(\"File downloaded successfully and saved as 'company_tickers.json'\")\n",
    "        return company_tickers\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "company_tickers = get_company_tickers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9998"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(company_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "index_name = \"stocks\"\n",
    "namespace = \"stock-descriptions\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Processing\n",
    "# This is a slow process, so we will use a more efficient method later\n",
    "\n",
    "# for idx, stock in company_tickers.items():\n",
    "#     stock_ticker = stock['ticker']\n",
    "#     stock_data = get_stock_info(stock_ticker)\n",
    "#     stock_description = stock_data['Business Summary']\n",
    "\n",
    "#     print(f\"Processing stock {idx} / {len(company_tickers)} :\", stock_ticker)\n",
    "\n",
    "#     vectorstore_from_documents = PineconeVectorStore.from_documents(\n",
    "#         documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
    "#         embedding=hf_embeddings,\n",
    "#         index_name=index_name,\n",
    "#         namespace=namespace\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the existing successful tickers from the file\n",
    "# If you want to start over, delete the successful_tickers.txt file\n",
    "\n",
    "\n",
    "# Initialize tracking lists\n",
    "successful_tickers = []\n",
    "unsuccessful_tickers = []\n",
    "\n",
    "# Load existing successful/unsuccessful tickers\n",
    "try:\n",
    "    with open('successful_tickers.txt', 'r') as f:\n",
    "        successful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing successful tickers file found\")\n",
    "\n",
    "try:\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        unsuccessful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(unsuccessful_tickers)} unsuccessful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing unsuccessful tickers file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of CPU cores\n",
    "# This is important for parallel processing\n",
    "# Use all cores minus 1 as max_workers for parallel processing\n",
    "# The less cores you have, the more you should reduce the batch size\n",
    "# 35 is a good starting point for most machines with 8 cores -- 7 max_workers * 5 = batch size 35 for m1 macbook pros\n",
    "import os\n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing\n",
    "# This is a faster process, but it requires more memory\n",
    "\n",
    "def process_stock(stock_ticker: str) -> str:\n",
    "    \n",
    "    if stock_ticker in successful_tickers:\n",
    "        return f\"Already processed {stock_ticker}\"\n",
    "\n",
    "    try:\n",
    "        # Get and store stock data\n",
    "        stock_data = get_stock_info(stock_ticker)\n",
    "        if stock_data['Business Summary'] is None:\n",
    "            stock_data['Business Summary'] = \"No business summary available\"\n",
    "        stock_description = stock_data['Business Summary']\n",
    "\n",
    "        vectorstore_from_texts = PineconeVectorStore.from_documents(\n",
    "            documents=[Document(page_content=stock_description, metadata=stock_data)],\n",
    "            embedding=hf_embeddings,\n",
    "            index_name=index_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "\n",
    "        # Track success\n",
    "        with open('successful_tickers.txt', 'a') as f:\n",
    "            f.write(f\"{stock_ticker}\\n\")\n",
    "        successful_tickers.append(stock_ticker)\n",
    "\n",
    "        return f\"Processed {stock_ticker} successfully\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Track failure\n",
    "        with open('unsuccessful_tickers.txt', 'a') as f:\n",
    "            f.write(f\"{stock_ticker}\\n\")\n",
    "        unsuccessful_tickers.append(stock_ticker)\n",
    "\n",
    "        return f\"ERROR processing {stock_ticker}: {e}\"\n",
    "\n",
    "def parallel_process_stocks(tickers: list, batch_size=35, max_workers: int = 5) -> None:\n",
    "    for i in range(0, len(tickers), batch_size):\n",
    "        print(f\"\\nProcessing batch {i//batch_size + 1}\")\n",
    "        batch = tickers[i:i + batch_size]\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_ticker = {\n",
    "            executor.submit(process_stock, ticker): ticker\n",
    "            for ticker in batch\n",
    "        }\n",
    "\n",
    "        for future in concurrent.futures.as_completed(future_to_ticker):\n",
    "            ticker = future_to_ticker[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "\n",
    "            except Exception as exc:\n",
    "                print(f'ERROR processing {ticker}: {exc}')\n",
    "                continue\n",
    "\n",
    "# Prepare your tickers\n",
    "tickers_to_process = [company_tickers[num]['ticker'] for num in company_tickers.keys()]\n",
    "\n",
    "# Process them\n",
    "parallel_process_stocks(tickers_to_process, max_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tickers that were successfully processed from the unsuccessful_tickers.txt file\n",
    "for ticker in unsuccessful_tickers:\n",
    "  if ticker in successful_tickers:\n",
    "    print(f\"Removing {ticker} from unsuccessful_tickers.txt\")\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open('unsuccessful_tickers.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            if line.strip() != ticker:\n",
    "                f.write(line)\n",
    "\n",
    "successful_tickers = []\n",
    "unsuccessful_tickers = []\n",
    "\n",
    "# Load existing successful/unsuccessful tickers\n",
    "try:\n",
    "    with open('successful_tickers.txt', 'r') as f:\n",
    "        successful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(successful_tickers)} successful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing successful tickers file found\")\n",
    "\n",
    "try:\n",
    "    with open('unsuccessful_tickers.txt', 'r') as f:\n",
    "        unsuccessful_tickers = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(unsuccessful_tickers)} unsuccessful tickers\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No existing unsuccessful tickers file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://api.groq.com/openai/v1\",\n",
    "  api_key=groq_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_streamlit():\n",
    "  os.system(\"streamlit run app.py --server.port 8501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import yfinance as yf\n",
    "import concurrent.futures\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "index_name = \"stocks\"\n",
    "namespace = \"stock-descriptions\"\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=hf_embeddings)\n",
    "\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "pinecone_index = pc.Index(index_name)\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://api.groq.com/openai/v1\",\n",
    "  api_key=groq_api_key\n",
    ")\n",
    "\n",
    "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model.encode(text)\n",
    "\n",
    "def get_stock_info_all(symbol: str) -> dict:\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {os.getenv(\"YAHOO_ACCESS_TOKEN\")}'\n",
    "    }\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    data = yf.Ticker(symbol, session=session)\n",
    "\n",
    "    stock_info = data.info\n",
    "\n",
    "    return stock_info\n",
    "\n",
    "def format_filter_conditions(filter_conditions):\n",
    "    if not filter_conditions:\n",
    "        return \"\"\n",
    "        \n",
    "    formatted_filters = []\n",
    "    \n",
    "    for key, value in filter_conditions.items():\n",
    "        if isinstance(value, dict):\n",
    "           \n",
    "            for op, val in value.items():\n",
    "                operator_map = {\n",
    "                    \"$gte\": \"greater than or equal to\",\n",
    "                    \"$lte\": \"less than or equal to\",\n",
    "                    \"$gt\": \"greater than\",\n",
    "                    \"$lt\": \"less than\",\n",
    "                    \"$eq\": \"equals\",\n",
    "                    \"$in\": \"in\",\n",
    "                }\n",
    "                op_text = operator_map.get(op, op)\n",
    "                formatted_filters.append(f\"{key} is {op_text} {val}\")\n",
    "        else:\n",
    "            \n",
    "            formatted_filters.append(f\"{key}: {value}\")\n",
    "    \n",
    "    return \", \".join(formatted_filters)\n",
    "\n",
    "\n",
    "def HandleQuery(query, filter_conditions):\n",
    "    filter_conditions_string = format_filter_conditions(filter_conditions)\n",
    "   \n",
    "    raw_query_embedding = get_huggingface_embeddings(query)\n",
    "\n",
    "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=10, include_metadata=True, namespace=namespace,filter=filter_conditions if filter_conditions else None)\n",
    "\n",
    "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
    "\n",
    "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query + filter_conditions_string\n",
    "\n",
    "    system_prompt = f\"\"\"You are an expert at providing answers about stocks. Please answer my question provided.\n",
    "\n",
    "    When giving your response, please do not mention the context provided to you or the query.\n",
    "\n",
    "    Please provide a detailed answer to the question.\n",
    "\n",
    "    Please provide all of the answers that you receive from the context provided.\n",
    "\n",
    "    Please provide the answers from most relevant to least relevant.\n",
    "\n",
    "    Please provide the answer in a markdown format.\n",
    "\n",
    "    Please be consistent in the markdown format for all of your answers.\n",
    "\n",
    "    If no question is provided, please provide a list of all of the stocks that match the filters and their information.\n",
    "    \"\"\"\n",
    "\n",
    "    llm_response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": augmented_query}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    response = llm_response.choices[0].message.content\n",
    "    return response\n",
    "\n",
    "st.title('Stock Analysis')\n",
    "st.warning(\"Keep in mind that more detailed your query and filters are, the more relevant and accurate the results will be.\")\n",
    "\n",
    "st.write(\"You can use the following filters to narrow down the results:\")\n",
    "st.write(\"Market Cap and Volume will return results that are greater than or equal to the value you enter.\")\n",
    "industry = st.text_input('Industry:',)\n",
    "sector = st.text_input('Sector:',)\n",
    "market_cap = st.number_input(\n",
    "    'Market Cap:',\n",
    "    min_value=0,\n",
    "    max_value=1000000,\n",
    "    step=1\n",
    ")\n",
    "volume = st.number_input(\n",
    "    'Volume:',\n",
    "    min_value=0,\n",
    "    max_value=1000000,\n",
    "    step=1\n",
    ")\n",
    "\n",
    "st.write(\"Ask general questions about stocks:\")\n",
    "query = st.text_input('Ask About Stocks:',)\n",
    "\n",
    "filter = {\n",
    "    \"industry\": industry,\n",
    "    \"sector\": sector,\n",
    "    \"marketCap\": {\"$gte\": market_cap},\n",
    "    \"volume\": {\"$gte\": volume}\n",
    "}\n",
    "\n",
    "if st.button('Get Stock Info'):\n",
    "    st.write(f'Getting info for {query}...')\n",
    "    response = HandleQuery(query, filter)\n",
    "    \n",
    "    st.write(\"### Response:\")\n",
    "    st.write(response)\n",
    "    \n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    if not response:\n",
    "        st.error(\"No information found for this query.\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
